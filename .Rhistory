par(fig=c(0,1,0.05,0.55), new=TRUE)
#generating blue color bar
colors_blue<- mat.or.vec(length(seeds)+1, 3)
for (i in 1:length(seeds)){
colors_blue [i,] <- c(0,0,1/i) }
#Add color orange for the true model
colors_blue[i+1,] <- c(1,0.5,0)
# theta2 label's name
theta2_names =c()
for(i in 1:length(seeds)){
theta2_names[i] <- paste("theta2 estimation using MCMC seed",as.character(i))
}
#Add theta2 true value
theta2_names[i+1] <- "theta 2 true value"
plot(1:(min(2*k,k+500)),chain2[1:min(2*k,k+500),1],type="l",
xlab="iterations",ylab="value",col = rgb(0, 0, 1),ylim=c(-20,20))
for (i in 2:length(seeds)){
lines(1:min(2*k,k+500),chain2[1:min(2*k,k+500),i],type="l",
col=rgb(matrix(colors_blue[i,], ncol = 3)))}
abline(h = True_theta2, lwd = 2, lty = 1,col="orange");
legend("bottomright", theta2_names,
lty=1, lwd = 3, col = rgb(colors_blue))
V.lp=mat.or.vec(length(V.theta1), length(V.theta2))
for(i in 1:length(V.theta1 )){
for (j in 1:length(V.theta2 )){
V.lp[i,j] <- logp(c(V.theta1[i],V.theta2[j])) }}
par( mfrow= c(2,2))
par(fig=c(0,1,0.45,1),mar=c(4,4,4,4))
contour(V.theta1, V.theta2,
V.lp,levels = ceiling(seq(max(V.lp)-sd(V.lp),max(V.lp),sd(V.lp)/10)),
col = hcl.colors(11,"heat") ,lwd=2,xlab="theta1", ylab="theta2")
abline(h = V.theta2[100], lwd = 3,lty=2,col="darkgrey");
abline(v = V.theta1[100], lwd = 3,lty=2,col="darkgrey");
points(True_theta1,True_theta2,pch=4, lwd = 4,col="black")
points(mean(chain1[50000:130000,1]),mean(chain2[50000:130000,1]),
pch=4, lwd = 4,col="red")
legend("bottomright", c("log posterior contours", "theta1-fixed",
"theta2-fixed","True value","Best estimate using mcmc"),
pch=c(NA, NA,NA,4,4) ,lty=c(1,2,2,NA,NA),lwd=c(2,2,2,4,4),
col = c("darkred","darkgrey","darkgrey","black","red"),)
par(fig=c(0,0.5,0.05,0.47), new=TRUE)
plot(V.theta2,V.lp[101,], type="l",col="darkred", lwd=2, xlab="theta2",
ylab="log posterior")
abline(v = True_theta2, lwd = 3,lty=2,col="black");
legend("bottomright", c("log posterior for fixed theta1","True value"),
lty=c(1,2),lwd=c(2,2), col = c("darkred","black"))
par(fig=c(0.5,1,0.05,0.47), new=TRUE)
plot(V.theta1,V.lp[,111], type="l",col="darkred", lwd=2, xlab="theta1",
ylab="log posterior")
abline(v = True_theta1, lwd = 3,lty=2,col="black");
legend("bottomleft", c("log posterior for fixed theta2","True value"),
lty=c(1,2),lwd=c(2,2), col = c("darkred","black"))
#SR: Generate random realizations from the fit curves
#using last 10000 MCMC simulations
Fit_model_MCMC<- mat.or.vec(10000, length(X))
for(j in 1:10000){
for (i in 1:length(X)){
Fit_model_MCMC[j,i] <- Y(mcmc.chains[length(mcmc.chains)/2-j,1],
mcmc.chains[length(mcmc.chains)/2-j,2],X[i])+
rnorm(1,mean=0,sd=sigma_obs)
}
}
# Generate the fit curves using mean and 5-95%CI of the last 10000 MCMC simulations
Fit.mcmc.CI = mat.or.vec(length(X), 2)
Fit.mcmc.mean = c()
for(i in 1:length(X)){
Fit.mcmc.CI[i,] <- quantile(Fit_model_MCMC[,i], probs = c(.05, .95))
Fit.mcmc.mean[i] <- mean(Fit_model_MCMC[,i])
}
plot(X, Fit.mcmc.mean, col="red", type="l",lty=2,lwd=2,ylim=c(-300,30),
xlim=c(-4.5,4.5) , xlab="t",
ylab="y")
lines(X, Fit.mcmc.CI[,1], col="grey", lty=2, lwd=2)
lines(X, Fit.mcmc.CI[,2], col="grey", lty=2,lwd=2)
lines(X, True_Y, col="blue", lwd=2)
points(t_obs,Observation, type="p",xlab="t",
ylab="y",col="black",lwd=3)
legend("bottomright", c("Observations","True model",
"Best Fit-mean mcmc","5-95% ( CI-mcmc + Noise )"),
lty=c(NA,1,2,2),lwd=c(2,2,2,2), pch=c(1,NA,NA,NA),
col=c("black","blue","red","grey"))
dev.off()
plot(X, Fit.mcmc.mean, col="red", type="l",lty=2,lwd=2,ylim=c(-300,30),
xlim=c(-4.5,4.5) , xlab="t",
ylab="y")
lines(X, Fit.mcmc.CI[,1], col="grey", lty=2, lwd=2)
lines(X, Fit.mcmc.CI[,2], col="grey", lty=2,lwd=2)
lines(X, True_Y, col="blue", lwd=2)
points(t_obs,Observation, type="p",xlab="t",
ylab="y",col="black",lwd=3)
legend("bottomright", c("Observations","True model",
"Best Fit-mean mcmc","5-95% ( CI-mcmc + Noise )"),
lty=c(NA,1,2,2),lwd=c(2,2,2,2), pch=c(1,NA,NA,NA),
col=c("black","blue","red","grey"))
par( mfrow= c(2,1))
par(fig=c(0,1,0.5,1),mar=c(4,4,4,4))
# label's name
names =c()
for(i in 1:length(seeds)){
names[i] <- paste("seed",as.character(i))
}
#Add true value
names[i+1] <- "True value"
# Plot
plot(density(chain1[50000:130000,1]),main="",xlab ="theta1",xlim=c(-12,-9),
ylim=c(0,1.5),col = rgb(1, 0, 0))
for(i in 2:length(seeds)) {
lines(density(chain1[50000:130000,i]),col = rgb(matrix(colors_red[i,],
ncol = 3)))}
abline(v = True_theta1, lwd = 2, lty = 1,col="orange");
legend("topright", names,
lty=1, lwd = 3, col = rgb(colors_red))
par(fig=c(0,1,0.05,0.55), new=TRUE)
plot(density(chain2[50000:130000,1]),main="",xlab ="theta2",xlim=c(-3,5),
ylim=c(0,0.6),col = rgb(0, 0, 1))
for(i in 2:length(seeds)) {
lines(density(chain2[50000:130000,i]),col = rgb(matrix(colors_blue[i,], ncol = 3)))}
abline(v = True_theta2, lwd = 2, lty = 1,col="orange");
legend("topright", names,
lty=1, lwd = 3, col = rgb(colors_blue))
set.seed(314)
chain1_sample=c()
chain2_sample=c()
for(i in 1:length(boot.par[,1])){
ind <- sample(50000:NI,1) #SR: only sample steps after burn-in
chain1_sample[i] <- chain1[ind,1]
chain2_sample[i] <- chain2[ind,1]
}
par( mfrow= c(2,1))
par(fig=c(0,1,0.5,1),mar=c(4,4,4,4))
plot(density(chain1_sample),main="",xlab ="theta1",xlim=c(-12,-9),
ylim=c(0,1.5),col = "red")
lines(density(boot.par[,1]),col = "orange")
abline(v = True_theta1, lwd = 2, lty = 1,col="black");
legend("topleft", c("theta1 estimation using mcmc",
"theta1 estimation using bootstrapping",
"theta1 true value"),
lty=1, lwd = 3, col = c("red","orange","black"))
par(fig=c(0,1,0.05,0.55), new=TRUE)
plot(density(chain2_sample),main="",xlab ="theta2",xlim=c(-3,5),ylim=c(0,0.6),
col = "blue")
lines(density(boot.par[,2]),col = "cyan")
abline(v = True_theta2, lwd = 2, lty = 1,col="black");
legend("topleft", c("theta2 estimation using mcmc",
"theta2 estimation using bootstrapping","theta2 true value"),
lty=1, lwd = 3, col = c("blue","cyan","black"))
plot(density(chain1_sample),main="",xlab ="theta1",xlim=c(-12,-9),
ylim=c(0,1.5),col = "red")
lines(density(boot.par[,1]),col = "orange")
abline(v = True_theta1, lwd = 2, lty = 1,col="black");
legend("topleft", c("theta1 estimation using mcmc",
"theta1 estimation using bootstrapping",
"theta1 true value"),
lty=1, lwd = 3, col = c("red","orange","black"))
plot(density(chain1_sample),main="",xlab ="theta1",xlim=c(-12,-9),
ylim=c(0,1.5),col = "red")
lines(density(boot.par[,1]),col = "orange")
abline(v = True_theta1, lwd = 2, lty = 1,col="black");
legend("topleft", c("theta1 estimation using mcmc",
"theta1 estimation using bootstrapping",
"theta1 true value"),
lty=1, lwd = 3, col = c("red","orange","black"))
plot(density(chain2_sample),main="",xlab ="theta2",xlim=c(-3,5),ylim=c(0,0.6),
col = "blue")
lines(density(boot.par[,2]),col = "cyan")
abline(v = True_theta2, lwd = 2, lty = 1,col="black");
legend("topleft", c("theta2 estimation using mcmc",
"theta2 estimation using bootstrapping","theta2 true value"),
lty=1, lwd = 3, col = c("blue","cyan","black"))
plot(chain1_sample,chain2_sample,lwd=1,ylab ="theta2",xlab ="theta1",
col="blue",pch=20)
points(True_theta1,True_theta2,pch=4, lwd = 4,col="black")
legend("topleft", c("Estimation using mcmc","True value"),
pch=c(20,4), lty=c(NA,NA),lwd = c(1,3), col = c("blue","black"))
plot(density(chain1[(NI/2+1):NI,1]),main="",xlab ="theta1",xlim=c(-12,-9),
ylim=c(0,1.5),col = "turquoise",lwd=3)
lines(density(chain1[1:NI,1]),col = "purple",lwd=3)
legend("topright", c("second half","full chain"),
lty=1, lwd = 3, col = c("turquoise","purple"))
plot(density(chain1[5e3:1e4,1]),main="",xlab ="theta1",xlim=c(-12,-9),
ylim=c(0,1.5),col = "turquoise",lwd=3)
lines(density(chain1[1:1e4,1]),col = "purple",lwd=3)
legend("topright", c("second 5k","first 10k"),
lty=1, lwd = 3, col = c("turquoise","purple"))
# Clear away any existing variables or figures.
rm(list = ls())
graphics.off()
# Install and read in packages.
# install.packages("coda")
# install.packages("mcmc")
# install.packages("batchmeans")
library(coda)
library(mcmc)
library(batchmeans)
# Set the seed for random sampling.
# All seeds in this tutorial are arbitrary.
setwd("/Users/f007f8t/Documents/GitHub/Bayesian-Statistics-Class-Dartmouth")
if(!dir.exists("figures")) dir.create("figures")
set.seed(1)
# Read in some observations with non-correlated measurement error
# (independent and identically distributed assumption).
#the below file does not exist
data <- read.table("observations.txt", header=TRUE)
t <- data$time
observations <- data$observations
# Plot data.
#par(mfrow = c(1,1))
pdf(file="figures/time_vs_observations.pdf",width=7,height=5)
plot(t, observations, pch = 20, xlab = "Time", ylab = "Observations")
dev.off()
# Set up a simple linear equation as a physical model.
model <- function(parm,t){ # Inputs are parameters and length of data
model.p <- length(parm) # number of parameters in the physical model
alpha <- parm[1]
beta <- parm[2]
y.mod <- alpha*t + beta # This linear equation represents a simple physical model
return(list(mod.obs = y.mod, model.p = model.p))
}
# Sample function for calculating the root mean squared error given a set of
# parameters, a vector of time values, and a vector of observations.
fn <- function(parameters, t, obs){
alpha <- parameters[1]
beta <- parameters[2]
data <- alpha*t + beta
resid <- obs - data
rmse <- sqrt(mean(resid^2))
# return the root mean square error
return(rmse)
}
# Plug in random values for the parameters.
parameter_guess <- c(0.5, 2)
# Optimize the physical model to find initial starting values for parameters.
# For optim to print more information add the arguments:
# method = "L-BFGS-B", control=list(trace=6))
result <- optim(parameter_guess, fn, gr=NULL, t, observations)
start_alpha <- result$par[1]
start_beta <- result$par[2]
parameter <- c(start_alpha, start_beta)
# Use the optimized parameters to generate a fit to the data and
# calculate the residuals.
y.obs <- model(parameter,t)
res <- observations - y.obs$mod.obs
start_sigma <- sd(res)
#par(mfrow = c(1,1))
pdf("figures/time_vs_residuals.pdf",width=7,height=5)
plot(res, type = "l", ylab = "Residuals", xlab = "Time")
points(res, pch = 20)
abline(h = 0, lty = 2)
dev.off()
# Set up priors.
bound.lower <- c(-1, -1, 0)
bound.upper <- c( 1, 3, 1)
# Name the parameters and specify the number of physical model parameters (alpha and beta).
# sigma is a statistical parameter and will not be counted in the number.
parnames <- c("alpha", "beta", "sigma")
model.p <- 2
# Load the likelihood model for measurement errors
source("iid_obs_likelihood.R")
# Optimize the likelihood function to estimate initial starting values
p <- c(start_alpha, start_beta, start_sigma)
p0 <- c(0.3, 1, 0.6) # random guesses
p0 <- optim(p0, function(p) -log.post(p))$par
print(round(p0,4))
# Set the step size and number of iterations.
step <- c(0.001, 0.01, 0.01)
NI <- 1E3
# Run MCMC calibration.
mcmc.out <- metrop(log.post, p0, nbatch = NI, scale = step)
prechain <- mcmc.out$batch
# Print the acceptance rate as a percent.
acceptrate <- mcmc.out$accept * 100
cat("Accept rate =", acceptrate, "%\n")
# Identify the burn-in period and subtract it from the chains
burnin <- seq(1, 0.01*NI, 1)
mcmc.chains <- prechain[-burnin, ]
## Check #1: Trace Plots:
pdf(file=paste0("figures/traceplots_",NI,"iterations.pdf"),width=10,height=7)
par(mfrow = c(2,2))
for(i in 1:3){
plot(mcmc.chains[ ,i], type="l", main = ""
,
ylab = paste('Parameter = ', parnames[i], sep = ''), xlab = "Number of Runs")
}
dev.off()
## Check #2: Monte Carlo Standard Error:
# est: approximation of the mean
# se: estimates the MCMC standard error
bm_est <- bmmat(mcmc.chains)
print(bm_est)
#is the mcse small relative to the size of the estimate?
for(i in 1:length(parnames)){
print(paste0("ratio of Monte Carlo standard error to Monte Carlo estimate for ",parnames[i],": ", as.numeric(bm_est[i,2]/bm_est[i,1])))
}
# Evaluate the number of significant figures
# assume we want a 95% confidence interval
z <- 1.96
interval <- matrix(data = NA, nrow = length(parnames), ncol = 2,
dimnames = list(c(1:length(parnames)), c("lower_bound", "upper_bound")))
for(i in 1:length(parnames)){
half_width<- z * bm_est[i ,"se"]
interval[i,1] <- bm_est[i ,"est"] - half_width
interval[i,2] <- bm_est[i ,"est"] + half_width
}
for(i in 1:length(parnames)){
print(paste0(parnames[i]," estimate :",as.numeric(bm_est[i,1])))
print(paste0(parnames[i]," interval :",as.numeric(interval[i,1]),",",as.numeric(interval[i,2])))
}
# "we are strongly suggesting that an estimate of the Monte Carlo standard error
# should be used to assess simulation error and reported.
# Without an attached MCSE a point estimate should not be trusted" (Flegal, Haran, and Jones, 2008)
## Check #3: Heidelberger and Welch's convergence diagnostic:
heidel.diag(mcmc.chains, eps = 0.1, pvalue = 0.05)
## Check #4: Gelman and Rubin's convergence diagnostic:
set.seed(111)
p0 <- c(0.05, 1.5, 0.6) # Arbitrary choice.
mcmc.out2 <- metrop(log.post, p0, nbatch=NI, scale=step)
prechain2 <- mcmc.out2$batch
set.seed(1708)
p0 <- c(0.1, 0.9, 0.3) # Arbitrary choice.
mcmc.out3 <- metrop(log.post, p0, nbatch=NI, scale=step)
prechain3 <- mcmc.out3$batch
set.seed(1234)
p0 <- c(0.3, 1.1, 0.5) # Arbitrary choice.
mcmc.out4 <- metrop(log.post, p0, nbatch=NI, scale=step)
prechain4 <- mcmc.out4$batch
# The burn-in has already been subtracted from the first chain.
# Thus, the burn-in only needs to be subtracted from the three other
# chains mcmc1 <- mcmc2 <- mcmc3 <- shrink factor
mcmc1<- as.mcmc(mcmc.chains)
mcmc2<- as.mcmc(prechain2[-burnin,])
mcmc3<- as.mcmc(prechain3[-burnin,])
mcmc4<- as.mcmc(prechain4[-burnin,])
set.seed(1)
mcmc_chain_list<- mcmc.list(list(mcmc1,mcmc2,mcmc3,mcmc4))
gelman.diag(mcmc_chain_list)
pdf(file=paste0("figures/gelman_plot_",NI,"iterations.pdf"),width=10,height=7)
gelman.plot(mcmc_chain_list)
dev.off()
#all point estimates and upper limits of the scale reduction factor are >1.1, so
#this is evidence that the Markov chains have not converged
# Calculate the 90% highest posterior density CI.
# HPDinterval() requires an mcmc object; this was done in the code block above.
hpdi = HPDinterval(mcmc1, prob = 0.90)
# Create density plot of each parameter.
pdf(file=paste0("figures/HPDI_vs_equaltailCI_",NI,"iterations.pdf"),width=10,height=8)
par(mfrow = c(2,2))
for(i in 1:3){
# Create density plot.
p.dens = density(mcmc.chains[,i])
plot(p.dens, xlab = paste('Parameter ='
,
' ', parnames[i], sep = ''), main="")
# Add mean estimate.
abline(v = bm(mcmc.chains[,i])$est, lwd = 2)
# Add 90% equal-tail CI.
CI = quantile(mcmc.chains[,i], prob = c(0.05, 0.95))
lines(x = CI, y = rep(0, 2), lwd = 2)
points(x = CI, y = rep(0, 2), pch = 16)
# Add 90% highest posterior density CI.
lines(x = hpdi[i, ], y = rep(mean(p.dens$y), 2), lwd = 2, col = "red")
points(x = hpdi[i, ], y = rep(mean(p.dens$y), 2), pch = 16, col = "red")
}
dev.off()
#black is equal tail. red is high posterior density.
rm(list = ls())
graphics.off()
####################
list.of.packages <- c("BASS","lhs","caTools","Metrics","sensobol","plotrix",
"MASS","fields")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()
[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
# Loading required libraries
library(BASS)
library(lhs)
library(caTools)
library(Metrics)
library(sensobol)
library(plotrix)
library(MASS)
library(fields)
################################################################################
# Define a seed for reproducibility
set.seed(314)
################################################################################
# Define the toy example, here is a Quadratic function
Y <- function (theta1, theta2,t) {
return (theta1*(t^2)+theta2*t)}
################################################################################
# First assume we have a true model with known theta1 and theta2
True_theta1 <- -10
True_theta2 <- 2
################################################################################
# Create some observation data with random error from Gaussian distribution
N_obs <- 20
t_obs <- runif(N_obs, -5, 5)
sigma_obs <- 10
Observation<- c()
for (i in 1:length(t_obs)){
Observation[i] <- Y(True_theta1,True_theta2,t_obs[i])+rnorm(1,0,sigma_obs)
}
################################################################################
# Generate many true values
X <- seq(-5,5,by=0.01)
True_Y<- c()
for (i in 1:length(X)){
True_Y[i] <- Y(True_theta1,True_theta2,X[i])
}
################################################################################
###############################PRECALIBRATION###################################
################################################################################
#Get a lhs sample
#theta_1~ unif(-20, 0)
#theta_2 ~ unif(-20, 20)
theta1_range<- c(-20,0)
theta2_range<- c(-20,20)
nSamples<- 10000
nPars<- 2
##100 samples, 4 variables
set.seed(33)
lhs_0_1<- lhs::maximinLHS(nSamples,nPars)
################################################################################
# Clear any existing variables and plots.
rm(list = ls())
graphics.off()
getwd()
list.of.packages <- c("BASS","lhs","caTools","Metrics","sensobol","plotrix",
"MASS","fields")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()
[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
# Loading required libraries
library(BASS)
library(lhs)
library(caTools)
library(Metrics)
library(sensobol)
library(plotrix)
library(MASS)
library(fields)
################################################################################
# Define a seed for reproducibility
set.seed(314)
################################################################################
# Define the toy example, here is a Quadratic function
Y <- function (theta1, theta2,t) {
return (theta1*(t^2)+theta2*t)}
################################################################################
# First assume we have a true model with known theta1 and theta2
True_theta1 <- -10
True_theta2 <- 2
################################################################################
# Create some observation data with random error from Gaussian distribution
N_obs <- 20
t_obs <- runif(N_obs, -5, 5)
sigma_obs <- 10
Observation<- c()
for (i in 1:length(t_obs)){
Observation[i] <- Y(True_theta1,True_theta2,t_obs[i])+rnorm(1,0,sigma_obs)
}
################################################################################
# Generate many true values
X <- seq(-5,5,by=0.01)
True_Y<- c()
for (i in 1:length(X)){
True_Y[i] <- Y(True_theta1,True_theta2,X[i])
}
#Get a lhs sample
#theta_1~ unif(-20, 0)
#theta_2 ~ unif(-20, 20)
theta1_range<- c(-20,0)
theta2_range<- c(-20,20)
nSamples<- 10000
nPars<- 2
##100 samples, 4 variables
set.seed(33)
lhs_0_1<- lhs::maximinLHS(nSamples,nPars)
#A 1000 by 2 Latin Hypercube Sample matrix with values uniformly distributed on [0,1]
#transform into samples from the right uniform distributions
lhs_sample<- matrix(0, nSamples, nPars)
lhs_sample[,1]<- qunif(lhs_0_1[,1], min = theta1_range[1], max = theta1_range[2])
lhs_sample[,2]<- qunif(lhs_0_1[,2], min = theta2_range[1], max = theta2_range[2])
colnames(lhs_sample)<- c("theta1","theta2")
################################################################################
#define an interval which model runs must be within to be considered 'behavioral'
behavioral_func<- function(sigma_obs, obs, model_eval){
return(model_eval > obs - 1.96*sigma_obs & model_eval < obs + 1.96*sigma_obs)
}
################################################################################
#get evaluate the function Y at all lhs samples of (theta_1, theta_2) and
#all t_obs values
#create a matrix where the row corresponds to the lhs sample
#and the column corresponds to the t_obs value, then
#evaluate for which lhs samples all function evaluations are within the
#uncertainty bounds for all observations
in_bds<- matrix(NA, nrow= nrow(lhs_sample), ncol= length(Observation))
for(i in 1:nrow(lhs_sample)){
for(j in 1:length(t_obs)){
in_bds[i,j]<- behavioral_func(sigma_obs, Observation[j], Y(lhs_sample[i,1],lhs_sample[i,2],t_obs[j]))
}
}
tot_in_bds<- rowSums(in_bds)
good_inds<- which(tot_in_bds==length(Observation))
good_samples<- lhs_sample[good_inds,]
#estimate the expected values of theta_1 and theta_2
e_theta1<- mean(lhs_sample[good_inds,1])
e_theta2<- mean(lhs_sample[good_inds,2])
maxSamples<- 1.3e5
nAdd<- 10000
e_theta1_chain<- e_theta1
e_theta2_chain<- e_theta2
test_ss<- seq(nSamples+nAdd,maxSamples,by=nAdd)
